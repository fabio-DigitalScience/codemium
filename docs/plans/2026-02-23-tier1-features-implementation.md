# Tier 1 Features Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Add three features to codemium: (1) always-on vendor/generated file filtering via go-enry, (2) per-repo license detection via go-license-detector, (3) opt-in code churn/hotspot analysis via provider REST APIs.

**Architecture:** Feature 1 modifies the analyzer's file walk to skip vendor/generated files using go-enry. Feature 2 adds license detection in the per-repo worker function after analysis, on the same cloned directory. Feature 3 adds a new `--churn` pass (like `--ai-estimate`) that fetches commit file-level stats from provider APIs and computes churn/hotspot scores.

**Tech Stack:** `go-enry/go-enry/v2` (vendor/generated detection), `go-enry/go-license-detector/v4` (license detection), existing `provider.CommitLister` pattern (churn).

---

### Task 1: Add go-enry dependency

**Files:**
- Modify: `go.mod`
- Modify: `go.sum`

**Step 1: Add the dependency**

Run: `go get github.com/go-enry/go-enry/v2`

**Step 2: Verify it resolves**

Run: `go mod tidy`

**Step 3: Commit**

```bash
git add go.mod go.sum
git commit -m "deps: add go-enry/go-enry/v2 for vendor/generated detection"
```

---

### Task 2: Add FilteredFiles field to model

**Files:**
- Modify: `internal/model/model.go`

**Step 1: Add the field to RepoStats**

In `internal/model/model.go`, add `FilteredFiles` to `RepoStats`:

```go
type RepoStats struct {
	Repository    string          `json:"repository"`
	Project       string          `json:"project,omitempty"`
	Provider      string          `json:"provider"`
	URL           string          `json:"url"`
	Languages     []LanguageStats `json:"languages"`
	Totals        Stats           `json:"totals"`
	FilteredFiles int64           `json:"filtered_files,omitempty"`
	AIEstimate    *AIEstimate     `json:"ai_estimate,omitempty"`
}
```

**Step 2: Add FilteredFiles to Stats (for report totals)**

```go
type Stats struct {
	Repos         int   `json:"repos,omitempty"`
	Files         int64 `json:"files"`
	Lines         int64 `json:"lines"`
	Code          int64 `json:"code"`
	Comments      int64 `json:"comments"`
	Blanks        int64 `json:"blanks"`
	Complexity    int64 `json:"complexity"`
	FilteredFiles int64 `json:"filtered_files,omitempty"`
}
```

**Step 3: Verify compilation**

Run: `go build ./...`

**Step 4: Run tests**

Run: `go test ./...`

**Step 5: Commit**

```bash
git add internal/model/model.go
git commit -m "feat: add FilteredFiles field to RepoStats and Stats"
```

---

### Task 3: Integrate go-enry filtering into analyzer

**Files:**
- Modify: `internal/analyzer/analyzer.go`
- Modify: `internal/analyzer/analyzer_test.go`

**Step 1: Write the failing test**

Add to `internal/analyzer/analyzer_test.go`:

```go
func TestAnalyzeFiltersVendorAndGenerated(t *testing.T) {
	dir := t.TempDir()

	// Real source file
	os.WriteFile(filepath.Join(dir, "main.go"), []byte("package main\n\nfunc main() {}\n"), 0644)

	// Vendor file — go-enry recognizes paths containing "vendor/"
	vendorDir := filepath.Join(dir, "vendor", "lib")
	os.MkdirAll(vendorDir, 0755)
	os.WriteFile(filepath.Join(vendorDir, "dep.go"), []byte("package lib\n\nfunc Dep() {}\n"), 0644)

	// Generated file — go-enry recognizes files with generation markers
	generated := "// Code generated by tool; DO NOT EDIT.\npackage gen\n\nfunc Gen() {}\n"
	os.WriteFile(filepath.Join(dir, "generated.go"), []byte(generated), 0644)

	a := analyzer.New()
	stats, err := a.Analyze(context.Background(), dir)
	if err != nil {
		t.Fatalf("analysis failed: %v", err)
	}

	// Only main.go should be counted
	if stats.Totals.Files != 1 {
		t.Errorf("expected 1 file (vendor+generated filtered), got %d", stats.Totals.Files)
	}

	if stats.FilteredFiles < 1 {
		t.Errorf("expected at least 1 filtered file, got %d", stats.FilteredFiles)
	}
}
```

**Step 2: Run test to verify it fails**

Run: `go test ./internal/analyzer/ -run TestAnalyzeFiltersVendorAndGenerated -v`
Expected: FAIL — vendor/generated files are currently counted

**Step 3: Implement filtering in analyzer.go**

Replace the directory skip block and add file-level filtering in `internal/analyzer/analyzer.go`. The changes:

1. Add import: `"github.com/go-enry/go-enry/v2"`
2. Replace hardcoded dir skip with `enry.IsVendor(path)` (this takes a path relative to root, so compute `relPath` via `strings.TrimPrefix`)
3. After reading file content, check `enry.IsGenerated(path, content)` and skip if true
4. Track `filteredFiles` counter, assign to `stats.FilteredFiles`

Updated `Analyze` method:

```go
func (a *Analyzer) Analyze(ctx context.Context, dir string) (*model.RepoStats, error) {
	langMap := map[string]*model.LanguageStats{}
	var totalFiles int64
	var filteredFiles int64

	err := filepath.Walk(dir, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return nil
		}
		if ctx.Err() != nil {
			return ctx.Err()
		}

		// Compute path relative to root for enry checks
		relPath, _ := filepath.Rel(dir, path)

		if info.IsDir() {
			if info.Name() == ".git" || info.Name() == ".hg" {
				return filepath.SkipDir
			}
			if enry.IsVendor(relPath + "/") {
				return filepath.SkipDir
			}
			return nil
		}

		if enry.IsVendor(relPath) {
			filteredFiles++
			return nil
		}

		content, err := os.ReadFile(path)
		if err != nil {
			return nil
		}

		if enry.IsGenerated(relPath, content) {
			filteredFiles++
			return nil
		}

		possibleLanguages, _ := processor.DetectLanguage(info.Name())
		if len(possibleLanguages) == 0 {
			return nil
		}

		job := &processor.FileJob{
			Filename:          info.Name(),
			Content:           content,
			Bytes:             int64(len(content)),
			PossibleLanguages: possibleLanguages,
		}

		job.Language = processor.DetermineLanguage(job.Filename, job.Language, job.PossibleLanguages, job.Content)
		if job.Language == "" {
			return nil
		}

		processor.CountStats(job)

		if job.Binary {
			return nil
		}

		lang, ok := langMap[job.Language]
		if !ok {
			lang = &model.LanguageStats{Name: job.Language}
			langMap[job.Language] = lang
		}

		lang.Files++
		lang.Lines += job.Lines
		lang.Code += job.Code
		lang.Comments += job.Comment
		lang.Blanks += job.Blank
		lang.Complexity += job.Complexity
		totalFiles++

		return nil
	})
	if err != nil {
		return nil, err
	}

	stats := &model.RepoStats{FilteredFiles: filteredFiles}
	for _, lang := range langMap {
		stats.Languages = append(stats.Languages, *lang)
		stats.Totals.Files += lang.Files
		stats.Totals.Lines += lang.Lines
		stats.Totals.Code += lang.Code
		stats.Totals.Comments += lang.Comments
		stats.Totals.Blanks += lang.Blanks
		stats.Totals.Complexity += lang.Complexity
	}

	return stats, nil
}
```

**Step 4: Run tests**

Run: `go test ./internal/analyzer/ -v`
Expected: All tests PASS

**Step 5: Run full test suite**

Run: `go test ./...`
Expected: All pass

**Step 6: Commit**

```bash
git add internal/analyzer/analyzer.go internal/analyzer/analyzer_test.go
git commit -m "feat: filter vendor/generated files using go-enry"
```

---

### Task 4: Propagate FilteredFiles through report building

**Files:**
- Modify: `cmd/codemium/main.go`

**Step 1: Add FilteredFiles aggregation in buildReport**

In `buildReport`, after the existing totals aggregation loop, add:

```go
report.Totals.FilteredFiles += r.Stats.FilteredFiles
```

This goes inside the `for _, r := range results` loop, in the non-error branch, alongside the other `report.Totals.X +=` lines.

**Step 2: Verify compilation and tests**

Run: `go build ./... && go test ./...`

**Step 3: Commit**

```bash
git add cmd/codemium/main.go
git commit -m "feat: aggregate FilteredFiles in report totals"
```

---

### Task 5: Display FilteredFiles in markdown output

**Files:**
- Modify: `internal/output/markdown.go`

**Step 1: Add FilteredFiles to Summary table**

After the Complexity row in `WriteMarkdown`, add:

```go
if report.Totals.FilteredFiles > 0 {
	fmt.Fprintf(w, "| Filtered Files | %d |\n", report.Totals.FilteredFiles)
}
```

**Step 2: Verify build**

Run: `go build ./...`

**Step 3: Commit**

```bash
git add internal/output/markdown.go
git commit -m "feat: show filtered file count in markdown summary"
```

---

### Task 6: Add go-license-detector dependency

**Files:**
- Modify: `go.mod`
- Modify: `go.sum`

**Step 1: Add the dependency**

Run: `go get github.com/go-enry/go-license-detector/v4`

**Step 2: Tidy**

Run: `go mod tidy`

**Step 3: Commit**

```bash
git add go.mod go.sum
git commit -m "deps: add go-enry/go-license-detector/v4 for license detection"
```

---

### Task 7: Create license detection package

**Files:**
- Create: `internal/license/license.go`
- Create: `internal/license/license_test.go`

**Step 1: Write the failing test**

Create `internal/license/license_test.go`:

```go
package license_test

import (
	"os"
	"path/filepath"
	"testing"

	"github.com/dsablic/codemium/internal/license"
)

func TestDetectMIT(t *testing.T) {
	dir := t.TempDir()
	// Standard MIT license text (abbreviated but recognizable)
	mit := `MIT License

Copyright (c) 2024 Example

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
`
	os.WriteFile(filepath.Join(dir, "LICENSE"), []byte(mit), 0644)

	result := license.Detect(dir)
	if result != "MIT" {
		t.Errorf("expected MIT, got %q", result)
	}
}

func TestDetectNoLicense(t *testing.T) {
	dir := t.TempDir()
	// No license file at all
	os.WriteFile(filepath.Join(dir, "main.go"), []byte("package main\n"), 0644)

	result := license.Detect(dir)
	if result != "" {
		t.Errorf("expected empty string, got %q", result)
	}
}
```

**Step 2: Run test to verify it fails**

Run: `go test ./internal/license/ -v`
Expected: FAIL — package doesn't exist yet

**Step 3: Implement license.go**

Create `internal/license/license.go`:

```go
package license

import (
	"github.com/go-enry/go-license-detector/v4/licensedb"
	"github.com/go-enry/go-license-detector/v4/licensedb/filer"
)

const confidenceThreshold = 0.85

// Detect scans the directory for license files and returns the SPDX
// identifier of the most confident match, or empty string if none found.
func Detect(dir string) string {
	f, err := filer.FromDirectory(dir)
	if err != nil {
		return ""
	}

	results, err := licensedb.Detect(f)
	if err != nil {
		return ""
	}

	var bestID string
	var bestConf float32
	for id, match := range results {
		if match.Confidence > bestConf && match.Confidence >= confidenceThreshold {
			bestConf = match.Confidence
			bestID = id
		}
	}

	return bestID
}
```

**Step 4: Run tests**

Run: `go test ./internal/license/ -v`
Expected: PASS

**Step 5: Commit**

```bash
git add internal/license/license.go internal/license/license_test.go
git commit -m "feat: add license detection package using go-license-detector"
```

---

### Task 8: Add License field to model and integrate into worker

**Files:**
- Modify: `internal/model/model.go`
- Modify: `cmd/codemium/main.go`

**Step 1: Add License field to RepoStats**

In `internal/model/model.go`, add to `RepoStats`:

```go
type RepoStats struct {
	Repository    string          `json:"repository"`
	Project       string          `json:"project,omitempty"`
	Provider      string          `json:"provider"`
	URL           string          `json:"url"`
	Languages     []LanguageStats `json:"languages"`
	Totals        Stats           `json:"totals"`
	License       string          `json:"license,omitempty"`
	FilteredFiles int64           `json:"filtered_files,omitempty"`
	AIEstimate    *AIEstimate     `json:"ai_estimate,omitempty"`
}
```

**Step 2: Call license.Detect in the analyze worker**

In `cmd/codemium/main.go`, in the `runAnalyze` worker function (the closure passed to `worker.RunWithProgress`), after `codeAnalyzer.Analyze(ctx, dir)` and before setting `stats.Repository`, add:

```go
stats.License = license.Detect(dir)
```

Add import: `"github.com/dsablic/codemium/internal/license"`

**Step 3: Verify compilation and tests**

Run: `go build ./... && go test ./...`

**Step 4: Commit**

```bash
git add internal/model/model.go cmd/codemium/main.go
git commit -m "feat: detect license per repo during analysis"
```

---

### Task 9: Display License in markdown output

**Files:**
- Modify: `internal/output/markdown.go`

**Step 1: Add License column to Repositories table**

In `WriteMarkdown`, modify the Repositories section to include a License column. Update both the AI and non-AI table variants:

For the non-AI case:
```go
fmt.Fprintf(w, "| Repository | Project | License | Files | Code | Comments | Complexity |\n")
fmt.Fprintf(w, "|------------|---------|---------|------:|-----:|---------:|-----------:|\n")
```

For the AI case:
```go
fmt.Fprintf(w, "| Repository | Project | License | Files | Code | Comments | Complexity | AI Commits %% | AI Additions |\n")
fmt.Fprintf(w, "|------------|---------|---------|------:|-----:|---------:|-----------:|-------------:|-------------:|\n")
```

And in the row rendering, insert `repo.License` (use "—" if empty) after Project:

```go
lic := repo.License
if lic == "" {
	lic = "—"
}
```

**Step 2: Verify build**

Run: `go build ./...`

**Step 3: Commit**

```bash
git add internal/output/markdown.go
git commit -m "feat: show license column in markdown repositories table"
```

---

### Task 10: Add ChurnStats and FileChurn to model

**Files:**
- Modify: `internal/model/model.go`

**Step 1: Add churn types**

Add to `internal/model/model.go`:

```go
// FileChurn holds churn metrics for a single file.
type FileChurn struct {
	Path       string  `json:"path"`
	Changes    int64   `json:"changes"`
	Additions  int64   `json:"additions"`
	Deletions  int64   `json:"deletions"`
	Complexity int64   `json:"complexity,omitempty"`
	Hotspot    float64 `json:"hotspot,omitempty"`
}

// ChurnStats holds code churn and hotspot data for a repository.
type ChurnStats struct {
	TotalCommits int64       `json:"total_commits"`
	TopFiles     []FileChurn `json:"top_files"`
	Hotspots     []FileChurn `json:"hotspots,omitempty"`
}
```

**Step 2: Add Churn field to RepoStats**

```go
type RepoStats struct {
	Repository    string          `json:"repository"`
	Project       string          `json:"project,omitempty"`
	Provider      string          `json:"provider"`
	URL           string          `json:"url"`
	Languages     []LanguageStats `json:"languages"`
	Totals        Stats           `json:"totals"`
	License       string          `json:"license,omitempty"`
	FilteredFiles int64           `json:"filtered_files,omitempty"`
	Churn         *ChurnStats     `json:"churn,omitempty"`
	AIEstimate    *AIEstimate     `json:"ai_estimate,omitempty"`
}
```

**Step 3: Verify compilation**

Run: `go build ./...`

**Step 4: Commit**

```bash
git add internal/model/model.go
git commit -m "feat: add ChurnStats and FileChurn model types"
```

---

### Task 11: Extend provider with CommitFileStats method

**Files:**
- Modify: `internal/provider/provider.go`
- Modify: `internal/provider/github.go`
- Modify: `internal/provider/github_test.go`
- Modify: `internal/provider/bitbucket.go`

**Step 1: Add FileChange type and ChurnLister interface**

In `internal/provider/provider.go`, add:

```go
// FileChange represents a file modified in a commit.
type FileChange struct {
	Path      string
	Additions int64
	Deletions int64
}

// ChurnLister extends Provider with per-file commit stats.
type ChurnLister interface {
	CommitLister
	CommitFileStats(ctx context.Context, repo model.Repo, hash string) ([]FileChange, error)
}
```

**Step 2: Write the failing test for GitHub**

Add to `internal/provider/github_test.go`:

```go
func TestGitHubCommitFileStats(t *testing.T) {
	server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		if r.URL.Path == "/repos/myorg/repo-1/commits/abc123" {
			json.NewEncoder(w).Encode(map[string]any{
				"sha": "abc123",
				"stats": map[string]any{
					"additions": 150,
					"deletions": 30,
				},
				"files": []map[string]any{
					{"filename": "main.go", "additions": 100, "deletions": 20},
					{"filename": "util.go", "additions": 50, "deletions": 10},
				},
			})
			return
		}
		w.WriteHeader(http.StatusNotFound)
	}))
	defer server.Close()

	gh := provider.NewGitHub("test-token", server.URL)
	files, err := gh.CommitFileStats(context.Background(), model.Repo{
		Slug: "repo-1",
		URL:  "https://github.com/myorg/repo-1",
	}, "abc123")
	if err != nil {
		t.Fatalf("CommitFileStats: %v", err)
	}
	if len(files) != 2 {
		t.Fatalf("expected 2 files, got %d", len(files))
	}
	if files[0].Path != "main.go" {
		t.Errorf("expected main.go, got %s", files[0].Path)
	}
	if files[0].Additions != 100 {
		t.Errorf("expected 100 additions, got %d", files[0].Additions)
	}
}
```

**Step 3: Run test to verify it fails**

Run: `go test ./internal/provider/ -run TestGitHubCommitFileStats -v`
Expected: FAIL — method doesn't exist

**Step 4: Implement CommitFileStats for GitHub**

Add to `internal/provider/github.go`. The GitHub commit detail endpoint already returns `files[]` — extend `githubCommitDetail` and add the method:

```go
type githubFileChange struct {
	Filename  string `json:"filename"`
	Additions int64  `json:"additions"`
	Deletions int64  `json:"deletions"`
}

// CommitFileStats fetches per-file changes for a single commit.
func (g *GitHub) CommitFileStats(ctx context.Context, repo model.Repo, hash string) ([]FileChange, error) {
	owner, name := ownerRepo(repo.URL)
	if owner == "" {
		return nil, fmt.Errorf("cannot parse owner/repo from URL: %s", repo.URL)
	}

	url := fmt.Sprintf("%s/repos/%s/%s/commits/%s", g.baseURL, owner, name, hash)
	req, err := http.NewRequestWithContext(ctx, http.MethodGet, url, nil)
	if err != nil {
		return nil, err
	}
	req.Header.Set("Authorization", "Bearer "+g.token)
	req.Header.Set("Accept", "application/vnd.github+json")

	resp, err := g.client.Do(req)
	if err != nil {
		return nil, fmt.Errorf("github commit detail API: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != http.StatusOK {
		return nil, fmt.Errorf("github commit detail API returned status %d", resp.StatusCode)
	}

	var detail struct {
		Files []githubFileChange `json:"files"`
	}
	if err := json.NewDecoder(resp.Body).Decode(&detail); err != nil {
		return nil, fmt.Errorf("decode github commit files: %w", err)
	}

	changes := make([]FileChange, len(detail.Files))
	for i, f := range detail.Files {
		changes[i] = FileChange{
			Path:      f.Filename,
			Additions: f.Additions,
			Deletions: f.Deletions,
		}
	}
	return changes, nil
}
```

**Step 5: Implement CommitFileStats for Bitbucket**

Add to `internal/provider/bitbucket.go`. The Bitbucket diffstat endpoint already returns per-file data — extend the existing types:

```go
type bitbucketDiffStatEntry struct {
	New *struct {
		Path string `json:"path"`
	} `json:"new"`
	Old *struct {
		Path string `json:"path"`
	} `json:"old"`
	LinesAdded   int64 `json:"lines_added"`
	LinesRemoved int64 `json:"lines_removed"`
}

// CommitFileStats fetches per-file changes for a single Bitbucket commit.
func (b *Bitbucket) CommitFileStats(ctx context.Context, repo model.Repo, hash string) ([]FileChange, error) {
	ws, slug := workspaceSlug(repo.URL)
	if ws == "" {
		return nil, fmt.Errorf("cannot parse workspace/slug from URL: %s", repo.URL)
	}

	var all []FileChange
	nextURL := fmt.Sprintf("%s/2.0/repositories/%s/%s/diffstat/%s",
		b.baseURL, url.PathEscape(ws), url.PathEscape(slug), url.PathEscape(hash))

	for nextURL != "" {
		resp, err := b.doGet(ctx, nextURL)
		if err != nil {
			return nil, fmt.Errorf("bitbucket diffstat API: %w", err)
		}

		if resp.StatusCode != http.StatusOK {
			resp.Body.Close()
			return nil, fmt.Errorf("bitbucket diffstat API returned status %d", resp.StatusCode)
		}

		var page struct {
			Values []bitbucketDiffStatEntry `json:"values"`
			Next   string                   `json:"next"`
		}
		if err := json.NewDecoder(resp.Body).Decode(&page); err != nil {
			resp.Body.Close()
			return nil, fmt.Errorf("decode bitbucket diffstat: %w", err)
		}
		resp.Body.Close()

		for _, d := range page.Values {
			path := ""
			if d.New != nil {
				path = d.New.Path
			} else if d.Old != nil {
				path = d.Old.Path
			}
			all = append(all, FileChange{
				Path:      path,
				Additions: d.LinesAdded,
				Deletions: d.LinesRemoved,
			})
		}
		nextURL = page.Next
	}

	return all, nil
}
```

**Step 6: Run tests**

Run: `go test ./internal/provider/ -v`
Expected: All pass

**Step 7: Commit**

```bash
git add internal/provider/provider.go internal/provider/github.go internal/provider/github_test.go internal/provider/bitbucket.go
git commit -m "feat: add CommitFileStats to provider for per-file churn data"
```

---

### Task 12: Create churn analysis package

**Files:**
- Create: `internal/churn/churn.go`
- Create: `internal/churn/churn_test.go`

**Step 1: Write the failing test**

Create `internal/churn/churn_test.go`:

```go
package churn_test

import (
	"context"
	"testing"

	"github.com/dsablic/codemium/internal/churn"
	"github.com/dsablic/codemium/internal/model"
	"github.com/dsablic/codemium/internal/provider"
)

type mockChurnLister struct {
	commits []provider.CommitInfo
	files   map[string][]provider.FileChange
}

func (m *mockChurnLister) ListCommits(_ context.Context, _ model.Repo, limit int) ([]provider.CommitInfo, error) {
	if limit > 0 && limit < len(m.commits) {
		return m.commits[:limit], nil
	}
	return m.commits, nil
}

func (m *mockChurnLister) CommitStats(_ context.Context, _ model.Repo, hash string) (int64, int64, error) {
	return 0, 0, nil
}

func (m *mockChurnLister) CommitFileStats(_ context.Context, _ model.Repo, hash string) ([]provider.FileChange, error) {
	return m.files[hash], nil
}

func TestAnalyzeChurn(t *testing.T) {
	mock := &mockChurnLister{
		commits: []provider.CommitInfo{
			{Hash: "aaa"},
			{Hash: "bbb"},
			{Hash: "ccc"},
		},
		files: map[string][]provider.FileChange{
			"aaa": {{Path: "main.go", Additions: 50, Deletions: 10}, {Path: "util.go", Additions: 20, Deletions: 5}},
			"bbb": {{Path: "main.go", Additions: 30, Deletions: 5}},
			"ccc": {{Path: "main.go", Additions: 10, Deletions: 2}, {Path: "README.md", Additions: 5, Deletions: 0}},
		},
	}

	repo := model.Repo{Slug: "test-repo", URL: "https://github.com/org/test-repo"}
	stats, err := churn.Analyze(context.Background(), mock, repo, 0)
	if err != nil {
		t.Fatalf("Analyze: %v", err)
	}

	if stats.TotalCommits != 3 {
		t.Errorf("expected 3 total commits, got %d", stats.TotalCommits)
	}

	if len(stats.TopFiles) == 0 {
		t.Fatal("expected at least 1 top file")
	}

	// main.go should be #1 (3 commits)
	if stats.TopFiles[0].Path != "main.go" {
		t.Errorf("expected main.go as top file, got %s", stats.TopFiles[0].Path)
	}
	if stats.TopFiles[0].Changes != 3 {
		t.Errorf("expected 3 changes for main.go, got %d", stats.TopFiles[0].Changes)
	}
}

func TestAnalyzeChurnLimit(t *testing.T) {
	mock := &mockChurnLister{
		commits: []provider.CommitInfo{
			{Hash: "aaa"},
			{Hash: "bbb"},
		},
		files: map[string][]provider.FileChange{
			"aaa": {{Path: "main.go", Additions: 50, Deletions: 10}},
			"bbb": {{Path: "main.go", Additions: 30, Deletions: 5}},
		},
	}

	repo := model.Repo{Slug: "test-repo", URL: "https://github.com/org/test-repo"}
	stats, err := churn.Analyze(context.Background(), mock, repo, 1)
	if err != nil {
		t.Fatalf("Analyze: %v", err)
	}

	if stats.TotalCommits != 1 {
		t.Errorf("expected 1 commit (limited), got %d", stats.TotalCommits)
	}
}
```

**Step 2: Run test to verify it fails**

Run: `go test ./internal/churn/ -v`
Expected: FAIL — package doesn't exist

**Step 3: Implement churn.go**

Create `internal/churn/churn.go`:

```go
package churn

import (
	"context"
	"sort"
	"sync"

	"github.com/dsablic/codemium/internal/model"
	"github.com/dsablic/codemium/internal/provider"
)

const (
	maxTopFiles    = 20
	statsConcurrency = 10
)

// Analyze computes churn statistics for a single repo by fetching
// per-file change data from commit history via the provider API.
func Analyze(ctx context.Context, cl provider.ChurnLister, repo model.Repo, commitLimit int) (*model.ChurnStats, error) {
	commits, err := cl.ListCommits(ctx, repo, commitLimit)
	if err != nil {
		return nil, err
	}

	// Fetch per-file stats for each commit concurrently
	type commitFiles struct {
		files []provider.FileChange
		err   error
	}

	results := make([]commitFiles, len(commits))
	sem := make(chan struct{}, statsConcurrency)
	var wg sync.WaitGroup

	for i, c := range commits {
		if ctx.Err() != nil {
			break
		}
		sem <- struct{}{}
		wg.Add(1)
		go func(idx int, hash string) {
			defer wg.Done()
			defer func() { <-sem }()
			files, err := cl.CommitFileStats(ctx, repo, hash)
			results[idx] = commitFiles{files: files, err: err}
		}(i, c.Hash)
	}
	wg.Wait()

	// Aggregate per-file churn
	type fileAgg struct {
		changes   int64
		additions int64
		deletions int64
	}
	agg := map[string]*fileAgg{}

	for _, r := range results {
		if r.err != nil {
			continue
		}
		for _, f := range r.files {
			a, ok := agg[f.Path]
			if !ok {
				a = &fileAgg{}
				agg[f.Path] = a
			}
			a.changes++
			a.additions += f.Additions
			a.deletions += f.Deletions
		}
	}

	// Build sorted top files
	var topFiles []model.FileChurn
	for path, a := range agg {
		topFiles = append(topFiles, model.FileChurn{
			Path:      path,
			Changes:   a.changes,
			Additions: a.additions,
			Deletions: a.deletions,
		})
	}

	sort.Slice(topFiles, func(i, j int) bool {
		return topFiles[i].Changes > topFiles[j].Changes
	})

	if len(topFiles) > maxTopFiles {
		topFiles = topFiles[:maxTopFiles]
	}

	return &model.ChurnStats{
		TotalCommits: int64(len(commits)),
		TopFiles:     topFiles,
	}, nil
}
```

**Step 4: Run tests**

Run: `go test ./internal/churn/ -v`
Expected: PASS

**Step 5: Commit**

```bash
git add internal/churn/churn.go internal/churn/churn_test.go
git commit -m "feat: add churn analysis package"
```

---

### Task 13: Add hotspot scoring (churn × complexity)

**Files:**
- Modify: `internal/churn/churn.go`
- Modify: `internal/churn/churn_test.go`

The hotspot calculation combines churn data with per-file complexity from the analyzer. Since the analyzer currently aggregates by language (not by file), we need to accept an optional complexity map.

**Step 1: Write the failing test**

Add to `internal/churn/churn_test.go`:

```go
func TestComputeHotspots(t *testing.T) {
	files := []model.FileChurn{
		{Path: "complex.go", Changes: 10, Additions: 500, Deletions: 100},
		{Path: "simple.go", Changes: 20, Additions: 200, Deletions: 50},
		{Path: "util.go", Changes: 5, Additions: 100, Deletions: 20},
	}
	complexity := map[string]int64{
		"complex.go": 50,
		"simple.go":  2,
		"util.go":    10,
	}

	hotspots := churn.ComputeHotspots(files, complexity, 10)

	if len(hotspots) != 3 {
		t.Fatalf("expected 3 hotspots, got %d", len(hotspots))
	}

	// complex.go should rank highest: 10 changes * 50 complexity = 500
	if hotspots[0].Path != "complex.go" {
		t.Errorf("expected complex.go as top hotspot, got %s", hotspots[0].Path)
	}
	if hotspots[0].Complexity != 50 {
		t.Errorf("expected complexity 50, got %d", hotspots[0].Complexity)
	}
	if hotspots[0].Hotspot != 500 {
		t.Errorf("expected hotspot score 500, got %f", hotspots[0].Hotspot)
	}
}
```

**Step 2: Run test to verify it fails**

Run: `go test ./internal/churn/ -run TestComputeHotspots -v`
Expected: FAIL — function doesn't exist

**Step 3: Implement ComputeHotspots**

Add to `internal/churn/churn.go`:

```go
const maxHotspots = 10

// ComputeHotspots takes churn data and per-file complexity, computes
// hotspot scores (changes * complexity), and returns top N sorted by score.
func ComputeHotspots(files []model.FileChurn, complexity map[string]int64, limit int) []model.FileChurn {
	if limit <= 0 {
		limit = maxHotspots
	}

	var hotspots []model.FileChurn
	for _, f := range files {
		c, ok := complexity[f.Path]
		if !ok || c == 0 {
			continue
		}
		hotspots = append(hotspots, model.FileChurn{
			Path:       f.Path,
			Changes:    f.Changes,
			Additions:  f.Additions,
			Deletions:  f.Deletions,
			Complexity: c,
			Hotspot:    float64(f.Changes) * float64(c),
		})
	}

	sort.Slice(hotspots, func(i, j int) bool {
		return hotspots[i].Hotspot > hotspots[j].Hotspot
	})

	if len(hotspots) > limit {
		hotspots = hotspots[:limit]
	}

	return hotspots
}
```

**Step 4: Run tests**

Run: `go test ./internal/churn/ -v`
Expected: All pass

**Step 5: Commit**

```bash
git add internal/churn/churn.go internal/churn/churn_test.go
git commit -m "feat: add hotspot scoring (churn × complexity)"
```

---

### Task 14: Add --churn flag and integrate into analyze command

**Files:**
- Modify: `cmd/codemium/main.go`

**Step 1: Add flags**

In `newAnalyzeCmd`, add:

```go
cmd.Flags().Bool("churn", false, "Analyze code churn and hotspots")
cmd.Flags().Int("churn-limit", 500, "Max commits to scan per repo for churn analysis (0 = unlimited)")
```

**Step 2: Add churn pass in runAnalyze**

After the AI estimation block (after line ~423 in `runAnalyze`), add a similar churn pass:

```go
churnFlag, _ := cmd.Flags().GetBool("churn")
churnLimit, _ := cmd.Flags().GetInt("churn-limit")

if churnFlag {
	churnLister, ok := prov.(provider.ChurnLister)
	if !ok {
		return fmt.Errorf("provider %s does not support churn analysis", providerName)
	}

	fmt.Fprintln(os.Stderr, "Analyzing code churn...")

	if useTUI {
		program = ui.RunTUI(len(repoList))
		go func() { program.Run() }()
	}

	churnProgressFn := func(completed, total int, repo model.Repo) {
		if useTUI && program != nil {
			program.Send(ui.ProgressMsg{
				Completed: completed,
				Total:     total,
				RepoName:  repo.Slug,
			})
		} else {
			fmt.Fprintf(os.Stderr, "[%d/%d] Churn %s\n", completed, total, repo.Slug)
		}
	}

	churnResults := worker.RunWithProgress(ctx, repoList, concurrency, func(ctx context.Context, repo model.Repo) (*model.RepoStats, error) {
		stats, err := churn.Analyze(ctx, churnLister, repo, churnLimit)
		if err != nil {
			return nil, err
		}
		return &model.RepoStats{
			Repository: repo.Slug,
			Churn:      stats,
		}, nil
	}, churnProgressFn)

	if useTUI && program != nil {
		program.Send(ui.DoneMsg{})
		time.Sleep(100 * time.Millisecond)
		program.Quit()
		program = nil
	}

	// Attach churn results to analysis results
	churnByRepo := make(map[string]*model.ChurnStats)
	for _, r := range churnResults {
		if r.Err == nil && r.Stats != nil && r.Stats.Churn != nil {
			churnByRepo[r.Repo.Slug] = r.Stats.Churn
		}
	}

	for i := range results {
		if results[i].Stats != nil {
			if cs, ok := churnByRepo[results[i].Repo.Slug]; ok {
				results[i].Stats.Churn = cs
			}
		}
	}
}
```

Add import: `"github.com/dsablic/codemium/internal/churn"`

**Step 3: Verify compilation**

Run: `go build ./...`

**Step 4: Run tests**

Run: `go test ./...`

**Step 5: Commit**

```bash
git add cmd/codemium/main.go
git commit -m "feat: add --churn flag to analyze command"
```

---

### Task 15: Display churn in markdown output

**Files:**
- Modify: `internal/output/markdown.go`

**Step 1: Add churn section to WriteMarkdown**

After the Repositories table section and before Errors, add a churn section:

```go
// Code Churn
var hasChurn bool
for _, repo := range report.Repositories {
	if repo.Churn != nil && len(repo.Churn.TopFiles) > 0 {
		hasChurn = true
		break
	}
}

if hasChurn {
	fmt.Fprintf(w, "## Code Churn\n\n")
	for _, repo := range report.Repositories {
		if repo.Churn == nil || len(repo.Churn.TopFiles) == 0 {
			continue
		}
		fmt.Fprintf(w, "### %s\n\n", repo.Repository)
		fmt.Fprintf(w, "**Commits scanned:** %d\n\n", repo.Churn.TotalCommits)

		fmt.Fprintf(w, "| File | Changes | Additions | Deletions |\n")
		fmt.Fprintf(w, "|------|--------:|----------:|----------:|\n")
		for _, f := range repo.Churn.TopFiles {
			fmt.Fprintf(w, "| %s | %d | %d | %d |\n",
				f.Path, f.Changes, f.Additions, f.Deletions)
		}
		fmt.Fprintln(w)

		if len(repo.Churn.Hotspots) > 0 {
			fmt.Fprintf(w, "**Hotspots** (high churn × high complexity):\n\n")
			fmt.Fprintf(w, "| File | Changes | Complexity | Hotspot Score |\n")
			fmt.Fprintf(w, "|------|--------:|-----------:|--------------:|\n")
			for _, h := range repo.Churn.Hotspots {
				fmt.Fprintf(w, "| %s | %d | %d | %.0f |\n",
					h.Path, h.Changes, h.Complexity, h.Hotspot)
			}
			fmt.Fprintln(w)
		}
	}
}
```

**Step 2: Verify build**

Run: `go build ./...`

**Step 3: Commit**

```bash
git add internal/output/markdown.go
git commit -m "feat: display churn and hotspots in markdown output"
```

---

### Task 16: Update documentation

**Files:**
- Modify: `CLAUDE.md`
- Modify: `README.md`
- Modify: `TODO.md`

**Step 1: Update CLAUDE.md**

- Add `internal/license/` and `internal/churn/` to Project Structure
- Update Architecture Notes to mention go-enry filtering and license detection
- Add `--churn` and `--churn-limit` to flag documentation
- Add `go-enry/go-enry/v2` and `go-enry/go-license-detector/v4` to Key Dependencies

**Step 2: Update README.md**

- Document new features: vendor filtering (always-on), license detection, churn analysis
- Add `--churn` and `--churn-limit` flags to usage section

**Step 3: Mark Tier 1 items as done in TODO.md**

Add checkmarks or "DONE" markers to items 1, 2, and 3.

**Step 4: Commit**

```bash
git add CLAUDE.md README.md TODO.md
git commit -m "docs: update documentation for Tier 1 features"
```

---

### Task 17: Final verification

**Step 1: Run full test suite**

Run: `go test ./...`
Expected: All pass

**Step 2: Build**

Run: `go build ./cmd/codemium`
Expected: Binary builds successfully

**Step 3: Run vet**

Run: `go vet ./...`
Expected: No issues
